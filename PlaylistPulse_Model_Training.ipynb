{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cb0935a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aagar8/Documents/DL_Song_Recommendation/dl_venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d41e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedSongDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for songs\"\"\"\n",
    "\n",
    "    def __init__(self, features, lyrics_embeddings):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.lyrics_embeddings = torch.tensor(lyrics_embeddings, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.features[idx], self.lyrics_embeddings[idx]), self.features[idx]\n",
    "\n",
    "\n",
    "class SimplerRecommenderNet(nn.Module):\n",
    "    \"\"\"Simplified Neural Network for Song Recommendation\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, lyrics_dim=768):\n",
    "        super().__init__()\n",
    "        combined_dim = input_dim + lyrics_dim\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, features, lyrics_embedding):\n",
    "        combined = torch.cat([features, lyrics_embedding], dim=1)\n",
    "        return self.network(combined)\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "\n",
    "class LyricProcessor:\n",
    "    \"\"\"Process lyrics using BERT\"\"\"\n",
    "\n",
    "    def __init__(self, cache_file=\"lyrics_embeddings_cache.pkl\"):\n",
    "        self.cache_file = cache_file\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"LyricProcessor using device: {self.device}\")\n",
    "\n",
    "        # Initialize BERT\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\").to(self.device)\n",
    "\n",
    "    def process_lyrics_batch(self, lyrics_list):\n",
    "        \"\"\"Process lyrics in batches\"\"\"\n",
    "        embeddings = []\n",
    "        batch_size = 32\n",
    "\n",
    "        # Ensure all lyrics are strings and clean them\n",
    "        cleaned_lyrics = []\n",
    "        for lyric in lyrics_list:\n",
    "            if isinstance(lyric, (float, int)):\n",
    "                lyric = str(lyric)\n",
    "            if not lyric or lyric.isspace():\n",
    "                lyric = \"no lyrics available\"\n",
    "            cleaned_lyrics.append(lyric)\n",
    "\n",
    "        for i in tqdm(range(0, len(cleaned_lyrics), batch_size), desc=\"Processing lyrics\"):\n",
    "            batch = cleaned_lyrics[i : i + batch_size]\n",
    "\n",
    "            # Ensure batch is a list of strings\n",
    "            batch = [str(text) for text in batch]\n",
    "\n",
    "            try:\n",
    "                inputs = self.tokenizer(\n",
    "                    batch,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=128,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "\n",
    "                # Move inputs to device\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.bert(**inputs)\n",
    "                    batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                    embeddings.extend(batch_embeddings)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch: {e}\")\n",
    "                # Add zero embeddings for failed batch\n",
    "                zero_embeddings = np.zeros((len(batch), 768))\n",
    "                embeddings.extend(zero_embeddings)\n",
    "\n",
    "        return np.array(embeddings)\n",
    "\n",
    "\n",
    "    def get_cached_embeddings(self, lyrics_list):\n",
    "        if os.path.exists(self.cache_file):\n",
    "            print(\"Loading cached embeddings...\")\n",
    "            with open(self.cache_file, \"rb\") as f:\n",
    "                cached_embeddings = pickle.load(f)\n",
    "                if len(cached_embeddings) == len(lyrics_list):\n",
    "                    return cached_embeddings\n",
    "                print(\"Cache size mismatch. Recomputing embeddings...\")\n",
    "\n",
    "        print(\"Computing BERT embeddings...\")\n",
    "        embeddings = self.process_lyrics_batch(lyrics_list)\n",
    "\n",
    "        print(\"Saving embeddings to cache...\")\n",
    "        with open(self.cache_file, \"wb\") as f:\n",
    "            pickle.dump(embeddings, f)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class FastSongRecommender:\n",
    "    def __init__(self, songs_df):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Recommender using device: {self.device}\")\n",
    "\n",
    "        self.songs_df = songs_df\n",
    "        self.preprocessor = None\n",
    "        self.model = None\n",
    "        self.lyric_processor = LyricProcessor()\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        print(\"Starting data preprocessing...\")\n",
    "\n",
    "        numerical_features = [\n",
    "            \"len\",\n",
    "            \"danceability\",\n",
    "            \"loudness\",\n",
    "            \"acousticness\",\n",
    "            \"instrumentalness\",\n",
    "            \"valence\",\n",
    "            \"energy\",\n",
    "            \"age\",\n",
    "            \"dating\",\n",
    "            \"violence\",\n",
    "            \"world/life\",\n",
    "            \"night/time\",\n",
    "            \"shake the audience\",\n",
    "            \"family/gospel\",\n",
    "            \"romantic\",\n",
    "            \"communication\",\n",
    "            \"obscene\",\n",
    "            \"music\",\n",
    "            \"movement/places\",\n",
    "            \"light/visual perceptions\",\n",
    "            \"family/spiritual\",\n",
    "            \"like/girls\",\n",
    "            \"sadness\",\n",
    "            \"feelings\",\n",
    "        ]\n",
    "\n",
    "        categorical_features = [\"genre\", \"topic\"]\n",
    "\n",
    "        # Clean and prepare lyrics\n",
    "        lyrics_list = self.songs_df[\"lyrics\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "        # Convert lyrics to list of strings and clean them\n",
    "        cleaned_lyrics = []\n",
    "        for lyric in lyrics_list:\n",
    "            # Clean and validate lyrics\n",
    "            if isinstance(lyric, (float, int)):\n",
    "                lyric = str(lyric)\n",
    "            if not lyric or lyric.isspace():\n",
    "                lyric = \"no lyrics available\"\n",
    "            cleaned_lyrics.append(lyric)\n",
    "\n",
    "        # Process lyrics\n",
    "        lyrics_embeddings = self.lyric_processor.get_cached_embeddings(self.songs_df[\"lyrics\"].fillna(\"\").astype(str).tolist())\n",
    "\n",
    "        # Create and fit preprocessor\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"num\", StandardScaler(), numerical_features),\n",
    "                (\n",
    "                    \"cat\",\n",
    "                    OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "                    categorical_features,\n",
    "                ),\n",
    "            ],\n",
    "            remainder=\"drop\",\n",
    "        )\n",
    "\n",
    "        # Process features\n",
    "        X = preprocessor.fit_transform(self.songs_df)\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "        # Ensure matching lengths\n",
    "        min_length = min(len(X), len(lyrics_embeddings))\n",
    "        X = X[:min_length]\n",
    "        lyrics_embeddings = lyrics_embeddings[:min_length]\n",
    "\n",
    "        print(\n",
    "            f\"Preprocessed data shapes - Features: {X.shape}, \"\n",
    "            f\"Lyrics embeddings: {lyrics_embeddings.shape}\"\n",
    "        )\n",
    "\n",
    "        return X, lyrics_embeddings\n",
    "    def save_model(self, model_path=\"model.pth\", preprocessor_path=\"preprocessor.pkl\"):\n",
    "        \"\"\"Save the trained model and preprocessor\"\"\"\n",
    "        # Save model state\n",
    "        if self.model is not None:\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model_state_dict\": self.model.state_dict(),\n",
    "                    \"input_dim\": self.model.network[0].in_features\n",
    "                    - 768,  # Save input dimension for reconstruction\n",
    "                },\n",
    "                model_path,\n",
    "            )\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "        else:\n",
    "            print(\"No model to save!\")\n",
    "\n",
    "        # Save preprocessor\n",
    "        if self.preprocessor is not None:\n",
    "            with open(preprocessor_path, \"wb\") as f:\n",
    "                pickle.dump(self.preprocessor, f)\n",
    "            print(f\"Preprocessor saved to {preprocessor_path}\")\n",
    "        else:\n",
    "            print(\"No preprocessor to save!\")\n",
    "\n",
    "\n",
    "    def load_model(self, model_path=\"model.pth\", preprocessor_path=\"preprocessor.pkl\"):\n",
    "        \"\"\"Load the trained model and preprocessor\"\"\"\n",
    "        if os.path.exists(model_path) and os.path.exists(preprocessor_path):\n",
    "            # Load preprocessor\n",
    "            with open(preprocessor_path, \"rb\") as f:\n",
    "                self.preprocessor = pickle.load(f)\n",
    "\n",
    "            # Load model\n",
    "            checkpoint = torch.load(model_path, map_location=self.device)\n",
    "            input_dim = checkpoint[\"input_dim\"]\n",
    "            self.model = SimplerRecommenderNet(input_dim=input_dim).to(self.device)\n",
    "            self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            self.model.eval()\n",
    "\n",
    "            print(\"Model and preprocessor loaded successfully!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Model or preprocessor files not found!\")\n",
    "            return False\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, test_size=0.2, random_state=42, num_epochs=30):\n",
    "        print(\"Starting training process...\")\n",
    "\n",
    "        # Preprocess data\n",
    "        X, lyrics_embeddings = self.preprocess_data()\n",
    "\n",
    "        # Split data\n",
    "        X_train, X_test, lyrics_train, lyrics_test = train_test_split(\n",
    "            X, lyrics_embeddings, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset = EnhancedSongDataset(X_train, lyrics_train)\n",
    "        test_dataset = EnhancedSongDataset(X_test, lyrics_test)\n",
    "\n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=64, shuffle=True, pin_memory=True, num_workers=4\n",
    "        )\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=64, shuffle=False, pin_memory=True, num_workers=4\n",
    "        )\n",
    "\n",
    "        # Initialize model\n",
    "        self.model = SimplerRecommenderNet(input_dim=X.shape[1]).to(self.device)\n",
    "\n",
    "        # Setup training\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.5, patience=3\n",
    "        )\n",
    "        early_stopping = EarlyStopping(patience=5)\n",
    "\n",
    "        # Training loop\n",
    "        history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "\n",
    "            progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "            for (features, lyrics_embed), targets in progress_bar:\n",
    "                features = features.to(self.device)\n",
    "                lyrics_embed = lyrics_embed.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(features, lyrics_embed)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                progress_bar.set_postfix({\"train_loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for (features, lyrics_embed), targets in test_loader:\n",
    "                    features = features.to(self.device)\n",
    "                    lyrics_embed = lyrics_embed.to(self.device)\n",
    "                    targets = targets.to(self.device)\n",
    "\n",
    "                    outputs = self.model(features, lyrics_embed)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            # Calculate average losses\n",
    "            train_loss /= len(train_loader)\n",
    "            val_loss /= len(test_loader)\n",
    "\n",
    "            # Update learning rate\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Check early stopping\n",
    "            early_stopping(val_loss)\n",
    "\n",
    "            # Save history\n",
    "            history[\"train_loss\"].append(train_loss)\n",
    "            history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                f\"Train Loss: {train_loss:.4f}, \"\n",
    "                f\"Val Loss: {val_loss:.4f}\"\n",
    "            )\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "        return history\n",
    "\n",
    "    def get_song_index(self, song_name=None, artist_name=None):\n",
    "        if song_name and artist_name:\n",
    "            matches = self.songs_df[\n",
    "                (self.songs_df[\"track_name\"].str.contains(song_name, case=False))\n",
    "                & (self.songs_df[\"artist_name\"].str.contains(artist_name, case=False))\n",
    "            ]\n",
    "        elif song_name:\n",
    "            matches = self.songs_df[\n",
    "                self.songs_df[\"track_name\"].str.contains(song_name, case=False)\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(\"Please provide at least a song name\")\n",
    "\n",
    "        if len(matches) == 0:\n",
    "            raise ValueError(\"No matching songs found\")\n",
    "\n",
    "        print(\"\\nMatching Songs:\")\n",
    "        print(matches[[\"artist_name\", \"track_name\", \"genre\"]])\n",
    "\n",
    "        return matches.index[0]\n",
    "\n",
    "    def recommend_similar_songs(self, song_index, top_k=5):\n",
    "        self.model.eval()\n",
    "        X, lyrics_embeddings = self.preprocess_data()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "            lyrics = torch.tensor(lyrics_embeddings, dtype=torch.float32).to(\n",
    "                self.device\n",
    "            )\n",
    "            embeddings = self.model(features, lyrics).cpu().numpy()\n",
    "\n",
    "        reference_embedding = embeddings[song_index]\n",
    "        similarities = np.dot(embeddings, reference_embedding) / (\n",
    "            np.linalg.norm(embeddings, axis=1) * np.linalg.norm(reference_embedding)\n",
    "        )\n",
    "\n",
    "        similar_indices = similarities.argsort()[::-1][1 : top_k + 1]\n",
    "        recommendations = self.songs_df.iloc[similar_indices].copy()\n",
    "        recommendations[\"similarity_score\"] = similarities[similar_indices]\n",
    "\n",
    "        return recommendations[\n",
    "            [\"artist_name\", \"track_name\", \"genre\", \"similarity_score\"]\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d471d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 27498 songs\n",
      "Recommender using device: cuda\n",
      "LyricProcessor using device: cuda\n",
      "Starting training process...\n",
      "Starting data preprocessing...\n",
      "Loading cached embeddings...\n",
      "Cache size mismatch. Recomputing embeddings...\n",
      "Computing BERT embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lyrics: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 860/860 [05:37<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving embeddings to cache...\n",
      "Preprocessed data shapes - Features: (27498, 39), Lyrics embeddings: (27498, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:02<00:00, 116.78it/s, train_loss=0.0638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Train Loss: 0.1478, Val Loss: 0.0267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:01<00:00, 298.56it/s, train_loss=0.0494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/30], Train Loss: 0.0562, Val Loss: 0.0216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:02<00:00, 138.91it/s, train_loss=0.0466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/30], Train Loss: 0.0474, Val Loss: 0.0169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:01<00:00, 173.61it/s, train_loss=0.0344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/30], Train Loss: 0.0424, Val Loss: 0.0158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:01<00:00, 272.06it/s, train_loss=0.0362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30], Train Loss: 0.0389, Val Loss: 0.0143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:02<00:00, 122.06it/s, train_loss=0.0343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/30], Train Loss: 0.0361, Val Loss: 0.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:01<00:00, 281.29it/s, train_loss=0.0309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/30], Train Loss: 0.0339, Val Loss: 0.0134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:01<00:00, 189.01it/s, train_loss=0.0364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/30], Train Loss: 0.0327, Val Loss: 0.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:02<00:00, 153.86it/s, train_loss=0.0312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/30], Train Loss: 0.0313, Val Loss: 0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:01<00:00, 268.70it/s, train_loss=0.0280]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30], Train Loss: 0.0303, Val Loss: 0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:02<00:00, 126.67it/s, train_loss=0.0289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/30], Train Loss: 0.0294, Val Loss: 0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:01<00:00, 237.97it/s, train_loss=0.0256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/30], Train Loss: 0.0282, Val Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:01<00:00, 270.31it/s, train_loss=0.0286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/30], Train Loss: 0.0276, Val Loss: 0.0105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:02<00:00, 121.42it/s, train_loss=0.0275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/30], Train Loss: 0.0269, Val Loss: 0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:01<00:00, 277.38it/s, train_loss=0.0240]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/30], Train Loss: 0.0265, Val Loss: 0.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:02<00:00, 145.24it/s, train_loss=0.0241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/30], Train Loss: 0.0259, Val Loss: 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:01<00:00, 192.57it/s, train_loss=0.0229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/30], Train Loss: 0.0256, Val Loss: 0.0102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:01<00:00, 274.87it/s, train_loss=0.0274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/30], Train Loss: 0.0252, Val Loss: 0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:02<00:00, 124.64it/s, train_loss=0.0212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/30], Train Loss: 0.0245, Val Loss: 0.0099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:01<00:00, 244.64it/s, train_loss=0.0238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/30], Train Loss: 0.0241, Val Loss: 0.0093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:01<00:00, 199.79it/s, train_loss=0.0246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/30], Train Loss: 0.0237, Val Loss: 0.0105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:02<00:00, 143.04it/s, train_loss=0.0239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/30], Train Loss: 0.0234, Val Loss: 0.0097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:01<00:00, 263.51it/s, train_loss=0.0232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/30], Train Loss: 0.0229, Val Loss: 0.0091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:02<00:00, 122.59it/s, train_loss=0.0226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/30], Train Loss: 0.0227, Val Loss: 0.0099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:01<00:00, 266.32it/s, train_loss=0.0238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/30], Train Loss: 0.0225, Val Loss: 0.0096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:02<00:00, 164.12it/s, train_loss=0.0204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/30], Train Loss: 0.0222, Val Loss: 0.0092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:01<00:00, 177.69it/s, train_loss=0.0217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/30], Train Loss: 0.0215, Val Loss: 0.0092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:01<00:00, 276.20it/s, train_loss=0.0184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/30], Train Loss: 0.0190, Val Loss: 0.0069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:02<00:00, 124.54it/s, train_loss=0.0146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/30], Train Loss: 0.0183, Val Loss: 0.0085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:01<00:00, 274.95it/s, train_loss=0.0162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/30], Train Loss: 0.0179, Val Loss: 0.0075\n",
      "Model saved to model.pth\n",
      "Preprocessor saved to preprocessor.pkl\n",
      "\n",
      "Enter song name (or 'quit' to exit): country boyz\n",
      "\n",
      "Matching Songs:\n",
      "       artist_name    track_name    genre\n",
      "27491  nappy roots  country boyz  hip hop\n",
      "Starting data preprocessing...\n",
      "Loading cached embeddings...\n",
      "Preprocessed data shapes - Features: (27498, 39), Lyrics embeddings: (27498, 768)\n",
      "\n",
      "Recommended Songs:\n",
      "        artist_name                              track_name    genre  \\\n",
      "27493       mack 10                         10 million ways  hip hop   \n",
      "22188     mr. vegas                            party tun up   reggae   \n",
      "27400   nappy roots                                  sholiz  hip hop   \n",
      "4614   citizen king  better days (and the bottom drops out)      pop   \n",
      "27386   nappy roots                            kentucky mud  hip hop   \n",
      "\n",
      "       similarity_score  \n",
      "27493          0.977242  \n",
      "22188          0.955152  \n",
      "27400          0.951693  \n",
      "4614           0.950117  \n",
      "27386          0.945063  \n",
      "\n",
      "Enter song name (or 'quit' to exit): quit\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load dataset\n",
    "    songs_df = pd.read_csv(\"cleaned_lyrics_data.csv\")\n",
    "    print(f\"Loaded dataset with {len(songs_df)} songs\")\n",
    "\n",
    "    # Validate required columns\n",
    "    required_columns = [\"lyrics\", \"artist_name\", \"track_name\", \"genre\", \"topic\"] + [\n",
    "        \"len\",\n",
    "        \"danceability\",\n",
    "        \"loudness\",\n",
    "        \"acousticness\",\n",
    "        \"instrumentalness\",\n",
    "        \"valence\",\n",
    "        \"energy\",\n",
    "        \"age\",\n",
    "        \"dating\",\n",
    "        \"violence\",\n",
    "        \"world/life\",\n",
    "        \"night/time\",\n",
    "        \"shake the audience\",\n",
    "        \"family/gospel\",\n",
    "        \"romantic\",\n",
    "        \"communication\",\n",
    "        \"obscene\",\n",
    "        \"music\",\n",
    "        \"movement/places\",\n",
    "        \"light/visual perceptions\",\n",
    "        \"family/spiritual\",\n",
    "        \"like/girls\",\n",
    "        \"sadness\",\n",
    "        \"feelings\",\n",
    "    ]\n",
    "\n",
    "    missing_columns = [col for col in required_columns if col not in songs_df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "    # Fill NaN values\n",
    "    songs_df = songs_df.fillna({\"lyrics\": \"\", \"genre\": \"unknown\", \"topic\": \"unknown\"})\n",
    "\n",
    "    # Initialize and train recommender\n",
    "    recommender = FastSongRecommender(songs_df)\n",
    "    history = recommender.train(num_epochs=30)\n",
    "    recommender.save_model()\n",
    "\n",
    "    # Get recommendations\n",
    "    while True:\n",
    "        song_name = input(\"\\nEnter song name (or 'quit' to exit): \")\n",
    "        if song_name.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            reference_song_index = recommender.get_song_index(song_name=song_name)\n",
    "            recommendations = recommender.recommend_similar_songs(reference_song_index)\n",
    "            print(\"\\nRecommended Songs:\")\n",
    "            print(recommendations)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_venv",
   "language": "python",
   "name": "dl_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
